import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import joblib # Wymagane do ≈Çadowania modeli .pkl (SVM)
from PIL import Image
from torchvision import transforms

# U≈ºywamy joblib, wiƒôc SVC i GradientBoostingClassifier nie sƒÖ ju≈º potrzebne do ≈Çadowania modelu,
# ale trzymamy je na wypadek, gdyby by≈Çy potrzebne gdzie indziej, choƒá usunƒôli≈õmy GB.
# W nowej wersji u≈ºywamy tylko klas architektonicznych z pliku treningowego.

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ====================================================================
# --- Definicje Architektury Nowego Modelu (SRM-CNN) ---
# Te klasy zosta≈Çy skopiowane z Twojego pliku treningowego
# ====================================================================

class SRMLayer(nn.Module):
    def __init__(self, in_channels=3):
        super().__init__()
        # Filtry SRM (krzy≈ºowe i kwadratowe)
        kv = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]], dtype=np.float32)
        kv3 = np.array([[-1, 2, -1], [2, -4, 2], [-1, 2, -1]], dtype=np.float32)
        kernels = [kv, kv3]

        conv_kernels_list = []
        for k in kernels:
            k = torch.from_numpy(k).float().unsqueeze(0).unsqueeze(0)
            conv_kernels_list.append(k)

        weight_per_channel = torch.cat(conv_kernels_list, dim=0)
        # Powtarzamy dla ka≈ºdego kana≈Çu wej≈õciowego (RGB)
        final_weight = weight_per_channel.repeat(in_channels, 1, 1, 1)

        self.register_buffer('kernels', final_weight)
        self.in_channels = in_channels
        self.n_kernels = len(kernels)
        self.kernel_size = kernels[0].shape[0]

    def forward(self, x):
        weight = self.kernels.to(x.device)
        padding = self.kernel_size // 2
        # Aplikujemy filtry SRM, wyj≈õcie = 2 * in_channels (3 * 2 = 6)
        out = F.conv2d(x, weight, padding=padding, groups=self.in_channels)
        return torch.tanh(out) # Funkcja aktywacji po filtracji

class SimpleStegNet(nn.Module):
    # CNN, kt√≥ry przetwarza wyj≈õcie z warstwy SRM (6 kana≈Ç√≥w)
    def __init__(self, in_channels=6, num_classes=2, feature_dim=512):
        super().__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        self.res_block1 = self._make_res_block(64, 128)
        self.res_block2 = self._make_res_block(128, 256)
        self.res_block3 = self._make_res_block(256, 512)
        self.pool = nn.AdaptiveAvgPool2d((1,1))
        self.fc = nn.Linear(512, feature_dim) # Warstwa do ekstrakcji cech dla SVM
        self.classifier = nn.Linear(feature_dim, num_classes)

    def _make_res_block(self, in_ch, out_ch):
        # Blok rezydualny/konwolucyjny z MaxPool
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2)
        )

    def forward(self, x):
        x = self.conv1(x)
        x = self.res_block1(x)
        x = self.res_block2(x)
        x = self.res_block3(x)
        x = self.pool(x).view(x.size(0), -1)
        feats = self.fc(x)
        logits = self.classifier(feats)
        return logits, feats

class FullStegModel(nn.Module):
    # Ca≈Çy model: SRM -> CNN
    def __init__(self, in_rgb_channels=3, num_classes=2, feature_dim=512):
        super().__init__()
        self.srm = SRMLayer(in_channels=in_rgb_channels)
        # Warstwa SRM generuje 6 kana≈Ç√≥w, wiƒôc SimpleStegNet jest inicjowany z in_channels=6
        self.net = SimpleStegNet(in_channels=in_rgb_channels * 2, num_classes=num_classes, feature_dim=feature_dim)

    def forward(self, x):
        residual = self.srm(x)
        logits, feats = self.net(residual)
        return logits, feats

# --- ≈öcie≈ºki do plik√≥w modeli ---
MODELS_DIR = os.path.join(os.path.dirname(__file__), "models")

# Zaktualizowane ≈õcie≈ºki do nowych modeli
CNN_PATH = os.path.join(MODELS_DIR, "best_stegnet_final.pth")
SVM_PATH = os.path.join(MODELS_DIR, "svm_stegano.pkl")

# Usuwamy ≈õcie≈ºki do GB i json√≥w z paramsami, bo sƒÖ niepotrzebne
# GB_PARAMS_PATH, SVM_PARAMS_PATH (usuwamy)

# --- Transformacja obrazu ---
# U≈ºywamy transformacji z pliku treningowego (IMG_SIZE=256), ale utrzymujemy
# Resize(320, 320) z oryginalnego pliku, aby zachowaƒá kompatybilno≈õƒá,
# chyba ≈ºe model by≈Ç trenowany na innym rozmiarze. W pliku treningowym by≈Ç 256.
# Zmieniƒô na 256, aby dopasowaƒá do modelu FullStegModel (kt√≥ry u≈ºywa 256 w treningu).
transform = transforms.Compose([
    transforms.Resize((256, 256)), # Dostosowane do rozmiaru treningowego
    transforms.ToTensor(),
    # Normalizacja z pliku treningowego
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

# ===============================
# üß† ≈Åadowanie modeli
# ===============================
try:
    # 1. ≈Åadowanie nowego modelu CNN (FullStegModel)
    cnn = FullStegModel(in_rgb_channels=3, num_classes=2, feature_dim=512).to(DEVICE)
    cnn.load_state_dict(torch.load(CNN_PATH, map_location=DEVICE))
    cnn.eval()
except Exception as e:
    print(f"B≈ÇƒÖd ≈Çadowania modelu CNN FullStegModel: {e}")
    # Mo≈ºesz dodaƒá logikƒô wyj≈õcia lub u≈ºycia domy≈õlnego modelu

try:
    # 2. ≈Åadowanie modelu SVM (zapisany jako .pkl za pomocƒÖ joblib)
    svm = joblib.load(SVM_PATH)
except Exception as e:
    print(f"B≈ÇƒÖd ≈Çadowania modelu SVM z joblib: {e}")
    # Mo≈ºesz dodaƒá logikƒô wyj≈õcia lub u≈ºycia domy≈õlnego modelu

# Gradient Boosting (gb) jest usuniƒôty, bo nowe ensemble to proste u≈õrednianie

# ===============================
# üîç Analiza pojedynczego obrazu
# ===============================
def analyze(pil_image: Image.Image):
    """
    Analizuje pojedynczy obraz PIL Image, u≈ºywajƒÖc nowego ensemble:
    FullStegModel (CNN) + SVM. Ensemble to proste u≈õrednienie prawdopodobie≈Ñstw.
    Zachowuje oryginalny format wyj≈õciowy.
    """
    try:
        img = pil_image.convert("RGB")
        # Przygotowanie tensora do predykcji
        tensor = transform(img).unsqueeze(0).to(DEVICE)

        with torch.no_grad():
            # 1. Predykcja CNN i ekstrakcja cech
            outputs, features = cnn(tensor)
            # CNN probability (P(klasa 1))
            cnn_prob = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()[0]
            features_np = features.cpu().numpy()

        # 2. Predykcja SVM na podstawie cech z CNN
        # SVM probability (P(klasa 1))
        svm_prob = svm.predict_proba(features_np)[:, 1][0]

        # 3. Nowa logika Ensemble: proste u≈õrednianie
        ensemble_prob = (cnn_prob + svm_prob) / 2.0

        # W pliku treningowym u≈ºywano optymalnego progu f1.
        # Aby zachowaƒá kompatybilno≈õƒá z poprzednim kodem, u≈ºywamy domy≈õlnego 0.5.
        threshold = 0.39
        detected = ensemble_prob >= threshold

        return {
            "method": "ensemble_Averaging", # Zaktualizowana nazwa metody
            "score": float(ensemble_prob),
            "detected": bool(detected),
            "details": {
                "cnn_prob": float(cnn_prob),
                "svm_prob": float(svm_prob),
                "ensemble_prob": float(ensemble_prob), # Wcze≈õniej by≈Ço to GB, teraz u≈õrednianie
                "threshold": threshold,
            },
        }

    except Exception as e:
        # Bardziej szczeg√≥≈Çowe logowanie b≈Çƒôdu, je≈õli to konieczne
        print(f"B≈ÇƒÖd w funkcji analyze: {e}")
        return {"error": str(e)}

if __name__ == '__main__':
    # Przyk≈Çadowy kod do testowania z obrazem zerowym (placeholder)
    print(f"UrzƒÖdzenie: {DEVICE}")
    print(f"CNN gotowe: {cnn.net.__class__.__name__}")
    print(f"SVM gotowe: {svm.__class__.__name__}")

    # Tworzenie pustego obrazu PIL jako placeholder do test√≥w
    try:
        dummy_image = Image.new('RGB', (256, 256), color = 'red')
        test_result = analyze(dummy_image)
        print("\n--- Wynik Analizy na Dummiesie ---")
        # Oczekiwany b≈ÇƒÖd, je≈õli model nie jest trenowany na tym obrazie,
        # ale testuje, czy interfejs dzia≈Ça.
        if "error" in test_result:
             print(f"Test interfejsu: Sukces (Zwr√≥cono b≈ÇƒÖd: {test_result['error']})")
        else:
             print(json.dumps(test_result, indent=4))
    except Exception as e:
        print(f"\nTEST B≈ÅƒÑD KRYTYCZNY: {e}")