{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1011a999",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install --quiet scikit-learn matplotlib torchvision pillow tqdm scikit-image\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.metrics import (roc_auc_score, roc_curve, precision_recall_curve,\n",
    "                             confusion_matrix, accuracy_score, classification_report,\n",
    "                             auc)\n",
    "import pandas as pd\n",
    "\n",
    "DRIVE_MOUNT_POINT = '/content/drive'\n",
    "COVER_DIR = '/content/drive/MyDrive/WIFD_dataset/STEGANO/cover'\n",
    "OUTPUT_ROOT = '/content/drive/MyDrive/stegano_dataset'  # Katalog zawierający już podkatalogi: cover, stego, pseudo\n",
    "IMAGE_SIZE = 256\n",
    "PATCH_SIZE = 128\n",
    "BATCH_SIZE = 12\n",
    "EMBED_DIM = 256\n",
    "NUM_EPOCHS = 15 # ZMIANA: Zwiększono z 8 do 15\n",
    "LR = 1e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SEED = 42\n",
    "MARGIN = 1.0\n",
    "MSE_THRESHOLD = 0.1\n",
    "# Zmienna usunięta lub zignorowana, ponieważ wprowadzamy stały limit par: NEG_PER_POS_RATIO = 1.0\n",
    "MAX_PAIRS_LIMIT = 1500 # ZMIANA: Zwiększono z 300 do 1500\n",
    "RESULTS_DIR = os.path.join(OUTPUT_ROOT, 'results_stego_vs_pseudo')\n",
    "SAVED_MODEL = os.path.join(RESULTS_DIR, 'siamese_stego_vs_pseudo_best.pth')\n",
    "METRICS_CSV = os.path.join(RESULTS_DIR, 'metrics.csv')\n",
    "SAMPLE_OUTPUT = os.path.join(RESULTS_DIR, 'samples') # Ten katalog teraz jest używany dynamicznie w podkatalogach epok\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ---- MOUNT DRIVE (Colab) ----\n",
    "from google.colab import drive\n",
    "try:\n",
    "    drive.mount(DRIVE_MOUNT_POINT)\n",
    "except Exception as e:\n",
    "    print('Mount drive (jeśli uruchamiasz lokalnie, upewnij się, że ścieżka istnieje):', e)\n",
    "\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "all_cover_paths = sorted(glob(os.path.join(OUTPUT_ROOT, 'cover', '*')))\n",
    "all_stego_paths = sorted(glob(os.path.join(OUTPUT_ROOT, 'stego', '*')))\n",
    "all_pseudo_paths = sorted(glob(os.path.join(OUTPUT_ROOT, 'pseudo', '*')))\n",
    "\n",
    "if len(all_cover_paths) == 0 or len(all_stego_paths) == 0 or len(all_pseudo_paths) == 0:\n",
    "    raise RuntimeError(\n",
    "        f'Nie znaleziono plików w gotowych katalogach! '\n",
    "        f'Oczekiwano danych w {OUTPUT_ROOT}/{{cover, stego, pseudo}}'\n",
    "    )\n",
    "\n",
    "print('Załadowano gotowe dane:')\n",
    "print(f'- Cover files: {len(all_cover_paths)}')\n",
    "print(f'- Stego files: {len(all_stego_paths)}')\n",
    "print(f'- Pseudo files: {len(all_pseudo_paths)}')\n",
    "\n",
    "\n",
    "def calculate_mse(img1_path, img2_path, size=IMAGE_SIZE):\n",
    "    \"\"\"Oblicza błąd średniokwadratowy (MSE) między dwoma obrazami w skali szarości.\"\"\"\n",
    "    try:\n",
    "        img1 = np.array(Image.open(img1_path).convert('L').resize((size, size)), dtype=np.float32) / 255.0\n",
    "        img2 = np.array(Image.open(img2_path).convert('L').resize((size, size)), dtype=np.float32) / 255.0\n",
    "    except Exception as e:\n",
    "        print(f\"Ostrzeżenie: Błąd ładowania lub przetwarzania pliku dla MSE: {e}. Zwracam dużą wartość MSE.\")\n",
    "        return 1.0\n",
    "\n",
    "    if img1.shape != img2.shape:\n",
    "        return 1.0\n",
    "\n",
    "    diff = img1 - img2\n",
    "    mse = np.mean(diff ** 2)\n",
    "    return mse\n",
    "\n",
    "def create_filtered_pairs(candidate_paths, label, max_limit):\n",
    "    \"\"\"\n",
    "    Tworzy pary (Cover, Candidate) filtrowane przez próg MSE i etykietowane.\n",
    "    Proces jest przerywany po osiągnięciu 'max_limit' par.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    desc = \"Tworzenie par Stego (Label 1)\" if label == 1 else \"Tworzenie par Pseudo (Label 0)\"\n",
    "\n",
    "    # KROK 1: Mieszanie, aby zapewnić, że wybrane 300 par jest losowych (nie tylko z pierwszych plików Cover)\n",
    "    cover_paths_shuffled = all_cover_paths.copy()\n",
    "    random.shuffle(cover_paths_shuffled)\n",
    "\n",
    "    # KROK 2: Iteracja z możliwością wczesnego przerwania\n",
    "    for c_path in tqdm(cover_paths_shuffled, desc=desc, total=len(cover_paths_shuffled)):\n",
    "\n",
    "        # Sprawdzanie i przerywanie głównej pętli\n",
    "        if len(pairs) >= max_limit:\n",
    "            break\n",
    "\n",
    "        c_name = Path(c_path).name\n",
    "        original_base_name = c_name.split('.')[0]\n",
    "        related_candidates = [\n",
    "            s for s in candidate_paths if Path(s).name.startswith(original_base_name)\n",
    "        ]\n",
    "\n",
    "        for s_path in related_candidates:\n",
    "            # Dodatkowe sprawdzenie, aby upewnić się, że nie przekroczymy limitu w jednej iteracji\n",
    "            if len(pairs) >= max_limit:\n",
    "                 break\n",
    "\n",
    "            mse = calculate_mse(c_path, s_path)\n",
    "\n",
    "            if mse < MSE_THRESHOLD:\n",
    "                pairs.append((c_path, s_path, label))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "pairs_positive = create_filtered_pairs(all_stego_paths, label=1, max_limit=MAX_PAIRS_LIMIT)\n",
    "print(f'Po MSE znaleziono par Stego (Label 1): {len(pairs_positive)}')\n",
    "\n",
    "pairs_negative = create_filtered_pairs(all_pseudo_paths, label=0, max_limit=MAX_PAIRS_LIMIT)\n",
    "print(f'Po MSE znaleziono par Pseudo (Label 0): {len(pairs_negative)}')\n",
    "\n",
    "\n",
    "print(f'Zbalansowane (ograniczone) pary: Stego (L1): {len(pairs_positive)} | Pseudo (L0): {len(pairs_negative)}')\n",
    "\n",
    "pairs = pairs_positive + pairs_negative\n",
    "random.shuffle(pairs)\n",
    "print('Łącznie par dla treningu/walidacji:', len(pairs))\n",
    "\n",
    "def make_srm_bank():\n",
    "    kernels = []\n",
    "    kernels += [np.array([[0,0,0],[0,1,-1],[0,0,0]]), np.array([[0,0,0],[0,1,0],[-1,0,0]]), np.array([[0,0,0],[0,1,0],[0,-1,0]])]\n",
    "    kernels += [np.array([[0,-1,0],[-1,4,-1],[0,-1,0]])]\n",
    "    base = kernels.copy()\n",
    "    for k in base:\n",
    "        kernels.append(np.rot90(k)); kernels.append(np.rot90(k,2))\n",
    "    kernels = kernels[:30]\n",
    "    arrs = []\n",
    "    for k in kernels:\n",
    "        kf = np.array(k, dtype=np.float32)\n",
    "        if kf.sum() != 0:\n",
    "            kf = kf - kf.mean()\n",
    "        arrs.append(kf)\n",
    "    bank = np.stack([k for k in arrs])[:, None, :, :]\n",
    "    return torch.from_numpy(bank)\n",
    "\n",
    "SRM_BANK = make_srm_bank().float()\n",
    "print('SRM bank (ilość filtrów):', SRM_BANK.shape[0])\n",
    "\n",
    "class PairedStegoDataset(Dataset):\n",
    "    def __init__(self, pairs, image_size=IMAGE_SIZE, patch_size=PATCH_SIZE, use_srm=True):\n",
    "        self.pairs = pairs\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.use_srm = use_srm\n",
    "        self.tf_full = T.Compose([T.Resize((self.image_size, self.image_size)), T.Grayscale(1), T.ToTensor()])\n",
    "        self.tf_patch = T.Compose([T.Lambda(lambda img: self.random_patch(img, self.patch_size)), T.Resize((self.patch_size,self.patch_size)), T.Grayscale(1), T.ToTensor()])\n",
    "\n",
    "    def random_patch(self, pil_img, size):\n",
    "        w,h = pil_img.size\n",
    "        if w < size or h < size:\n",
    "            return pil_img.resize((size,size))\n",
    "        x = random.randint(0, w-size)\n",
    "        y = random.randint(0, h-size)\n",
    "        return pil_img.crop((x,y,x+size,y+size))\n",
    "\n",
    "    def apply_srm(self, img_tensor):\n",
    "        device = img_tensor.device\n",
    "        bank = SRM_BANK.to(device)\n",
    "        pad = (bank.shape[2]//2, bank.shape[3]//2)\n",
    "        img = img_tensor.unsqueeze(0)\n",
    "        out = nn.functional.conv2d(img, bank, padding=pad)\n",
    "        return out.squeeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a_path, b_path, label = self.pairs[idx]\n",
    "\n",
    "        try:\n",
    "            a = Image.open(a_path).convert('RGB')\n",
    "        except Exception:\n",
    "            a = Image.new('RGB', (self.image_size, self.image_size), color = 'black')\n",
    "\n",
    "        try:\n",
    "            b = Image.open(b_path).convert('RGB')\n",
    "        except Exception:\n",
    "            b = Image.new('RGB', (self.image_size, self.image_size), color = 'black')\n",
    "\n",
    "        a_full = self.tf_full(a); b_full = self.tf_full(b)\n",
    "        a_patch = self.tf_patch(a); b_patch = self.tf_patch(b)\n",
    "\n",
    "        if self.use_srm:\n",
    "            a_full_srm = self.apply_srm(a_full); b_full_srm = self.apply_srm(b_full)\n",
    "            a_patch_srm = self.apply_srm(a_patch); b_patch_srm = self.apply_srm(b_patch)\n",
    "        else:\n",
    "            a_full_srm = a_full.repeat(SRM_BANK.shape[0],1,1)\n",
    "            b_full_srm = b_full.repeat(SRM_BANK.shape[0],1,1)\n",
    "            a_patch_srm = a_patch.repeat(SRM_BANK.shape[0],1,1)\n",
    "            b_patch_srm = b_patch.repeat(SRM_BANK.shape[0],1,1)\n",
    "\n",
    "        return (a_full_srm, a_patch_srm), (b_full_srm, b_patch_srm), torch.tensor(label, dtype=torch.float32), a_path, b_path\n",
    "\n",
    "# split\n",
    "train_pairs = pairs[:int(0.8*len(pairs))]\n",
    "val_pairs = pairs[int(0.8*len(pairs)):]\n",
    "train_ds = PairedStegoDataset(train_pairs)\n",
    "val_ds = PairedStegoDataset(val_pairs)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "print('Train/Val pairs:', len(train_ds), len(val_ds))\n",
    "\n",
    "class MultiScaleBranch(nn.Module):\n",
    "    def __init__(self, in_channels, embed_dim=EMBED_DIM):\n",
    "        super().__init__()\n",
    "        self.local = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64,128,3,padding=1,stride=2), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(256, embed_dim//2)\n",
    "        )\n",
    "        self.global_stream = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,32,5,padding=2), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32,64,3,padding=1,stride=2), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64,128,3,padding=1,stride=2), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(128, embed_dim//2)\n",
    "        )\n",
    "    def forward(self, full, patch):\n",
    "        g = self.global_stream(full)\n",
    "        l = self.local(patch)\n",
    "        return torch.cat([g,l], dim=1)\n",
    "\n",
    "class SiameseMulti(nn.Module):\n",
    "    def __init__(self, in_channels, embed_dim=EMBED_DIM):\n",
    "        super().__init__()\n",
    "        self.branch = MultiScaleBranch(in_channels, embed_dim=embed_dim)\n",
    "        self.head = nn.Sequential(nn.Linear(embed_dim,128), nn.ReLU(inplace=True), nn.Linear(128,1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, a_full,a_patch,b_full,b_patch):\n",
    "        ea = self.branch(a_full,a_patch)\n",
    "        eb = self.branch(b_full,b_patch)\n",
    "        d = torch.abs(ea - eb)\n",
    "        out = self.head(d).squeeze(1)\n",
    "        return out, ea, eb\n",
    "\n",
    "model = SiameseMulti(in_channels=SRM_BANK.shape[0], embed_dim=EMBED_DIM).to(DEVICE)\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=MARGIN):\n",
    "        super().__init__(); self.margin = margin\n",
    "    def forward(self, emb1, emb2, label):\n",
    "        d = torch.norm(emb1 - emb2, p=2, dim=1)\n",
    "        loss_pos = label * 0.5 * torch.clamp(self.margin - d, min=0.0)**2\n",
    "        loss_neg = (1.0 - label) * 0.5 * d**2\n",
    "        return (loss_pos + loss_neg).mean()\n",
    "\n",
    "criterion_contrast = ContrastiveLoss()\n",
    "criterion_bce = nn.BCELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def train_epoch(model, loader, opt):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (a_full,a_patch),(b_full,b_patch),labels,_,_ in tqdm(loader, desc=\"Trening\"):\n",
    "        a_full = a_full.to(DEVICE); a_patch = a_patch.to(DEVICE)\n",
    "        b_full = b_full.to(DEVICE); b_patch = b_patch.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        out, ea, eb = model(a_full,a_patch,b_full,b_patch)\n",
    "        loss_c = criterion_contrast(ea, eb, labels)\n",
    "        loss_b = criterion_bce(out, labels)\n",
    "        loss = loss_c + 0.5 * loss_b\n",
    "        loss.backward(); opt.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_model(model, loader):\n",
    "    model.eval()\n",
    "    ys=[]; preds=[]; emb_d=[]; paths=[]\n",
    "    with torch.no_grad():\n",
    "        for (a_full,a_patch),(b_full,b_patch),labels,a_paths,b_paths in tqdm(loader, desc=\"Walidacja\"):\n",
    "            a_full = a_full.to(DEVICE); a_patch = a_patch.to(DEVICE)\n",
    "            b_full = b_full.to(DEVICE); b_patch = b_patch.to(DEVICE)\n",
    "\n",
    "            out, ea, eb = model(a_full,a_patch,b_full,b_patch)\n",
    "\n",
    "            ys.extend(labels.numpy().tolist())\n",
    "            preds.extend(out.cpu().numpy().tolist())\n",
    "            emb_d.extend(((ea-eb).pow(2).sum(dim=1).sqrt()).cpu().numpy().tolist())\n",
    "            paths.extend(list(zip(a_paths, b_paths)))\n",
    "\n",
    "    return np.array(ys), np.array(preds), np.array(emb_d), paths\n",
    "\n",
    "def save_eval_artifacts(model, loader, ys_val, preds_val, val_paths, epoch, chosen_thr):\n",
    "    \"\"\"Generuje i zapisuje wszystkie metryki i wizualizacje do katalogu wyników.\"\"\"\n",
    "    # Obliczanie metryk\n",
    "    try:\n",
    "        auc_score = roc_auc_score(ys_val, preds_val)\n",
    "    except Exception:\n",
    "        auc_score = float('nan')\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(ys_val, preds_val)\n",
    "    precision, recall, pr_thr = precision_recall_curve(ys_val, preds_val)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    pred_labels = (preds_val >= chosen_thr).astype(int)\n",
    "    cm = confusion_matrix(ys_val, pred_labels)\n",
    "    acc = accuracy_score(ys_val, pred_labels)\n",
    "    report = classification_report(ys_val, pred_labels, digits=4)\n",
    "\n",
    "    metrics = {\n",
    "        'epoch':[epoch],\n",
    "        'val_auc_roc':[auc_score],\n",
    "        'val_auc_pr':[pr_auc],\n",
    "        'threshold':[chosen_thr],\n",
    "        'accuracy':[acc]\n",
    "    }\n",
    "    df_metrics = pd.DataFrame(metrics)\n",
    "    # Zapisz lub dołącz\n",
    "    if epoch == 1:\n",
    "        df_metrics.to_csv(METRICS_CSV, index=False)\n",
    "    else:\n",
    "        # Sprawdź, czy plik istnieje, aby uniknąć błędów, jeśli jest to pierwsza epoka\n",
    "        if os.path.exists(METRICS_CSV):\n",
    "            df_metrics.to_csv(METRICS_CSV, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            df_metrics.to_csv(METRICS_CSV, index=False)\n",
    "\n",
    "\n",
    "    # Upewnij się, że katalog dla epoki istnieje\n",
    "    epoch_dir = os.path.join(RESULTS_DIR, f'epoch_{epoch:02d}')\n",
    "    os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "    plt.figure(figsize=(6,6)); plt.plot(fpr, tpr, label=f'AUC ROC: {auc_score:.4f}'); plt.plot([0,1],[0,1],'--');\n",
    "    plt.xlabel('FPR (Fałszywie pozytywne)'); plt.ylabel('TPR (Prawdziwie pozytywne)');\n",
    "    plt.title(f'Krzywa ROC (Stego vs Pseudo) - Ep. {epoch}'); plt.grid(True); plt.legend();\n",
    "    plt.savefig(os.path.join(epoch_dir, 'roc_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6,4)); plt.plot(recall, precision, label=f'AUC PR: {pr_auc:.4f}');\n",
    "    plt.xlabel('Recall (Czułość)'); plt.ylabel('Precision (Precyzja)');\n",
    "    plt.title(f'Krzywa Precision-Recall - Ep. {epoch}'); plt.grid(True); plt.legend();\n",
    "    plt.savefig(os.path.join(epoch_dir, 'pr_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5,4)); plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues);\n",
    "    plt.title(f'Macierz pomyłek - Ep. {epoch}'); plt.colorbar();\n",
    "    plt.xticks([0,1], ['Pseudo (0)','Stego (1)'], rotation=45, ha='right'); plt.yticks([0,1], ['Pseudo (0)','Stego (1)'])\n",
    "    plt.ylabel('Prawdziwa etykieta'); plt.xlabel('Etykieta predykcji');\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, cm[i,j], ha='center', va='center', color='white' if cm[i,j]>cm.max()/2 else 'black')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(epoch_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6,4));\n",
    "    plt.hist(preds_val[ys_val==0], bins=50, alpha=0.6, label='negatyw (Pseudo)');\n",
    "    plt.hist(preds_val[ys_val==1], bins=50, alpha=0.6, label='pozytyw (Stego)');\n",
    "    plt.legend();\n",
    "    plt.title(f'Rozkład wyników predykcji - Ep. {epoch}');\n",
    "    plt.xlabel('Wynik predykcji');\n",
    "    plt.ylabel('Liczba próbek');\n",
    "    plt.savefig(os.path.join(epoch_dir, 'scores_histogram.png'))\n",
    "    plt.close()\n",
    "\n",
    "    with open(os.path.join(epoch_dir, 'classification_report.txt'),'w') as f:\n",
    "        f.write(f'--- Raport Klasyfikacji (Epoka {epoch}) ---\\n')\n",
    "        f.write(f'AUC ROC: {auc_score:.4f}\\n')\n",
    "        f.write(f'Accuracy: {acc:.4f}\\n')\n",
    "        f.write(f'Wybrany próg (Youden): {chosen_thr:.4f}\\n')\n",
    "        f.write('\\nClassification Report:\\n')\n",
    "        f.write(str(report))\n",
    "\n",
    "    print(f'Zapisano artefakty ewaluacyjne dla Epoki {epoch} do: {epoch_dir}')\n",
    "\n",
    "\n",
    "\n",
    "    # Czcionka (dla kompatybilności Colab)\n",
    "    font = None\n",
    "    try:\n",
    "        font_path = '/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf'\n",
    "        if os.path.exists(font_path):\n",
    "            font = ImageFont.truetype(font_path, 16)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    n_save = 10 # Zapisz 10 przykładów (5 najbardziej pewnych negatywów, 5 najbardziej pewnych pozytywów)\n",
    "    # Wybierz najbardziej pewne pozytywy (najwyższy wynik) i najbardziej pewne negatywy (najniższy wynik)\n",
    "    idxs = np.argsort(preds_val)[:n_save//2].tolist() + np.argsort(preds_val)[-n_save//2:].tolist()\n",
    "\n",
    "    for k, idx in enumerate(idxs):\n",
    "        a_path, b_path = val_paths[idx]\n",
    "        score = preds_val[idx]\n",
    "        lab = ys_val[idx]\n",
    "\n",
    "        try:\n",
    "            a = Image.open(a_path).convert('RGB').resize((IMAGE_SIZE,IMAGE_SIZE))\n",
    "            b = Image.open(b_path).convert('RGB').resize((IMAGE_SIZE,IMAGE_SIZE))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Połącz obraz A (Cover) i B (Stego/Pseudo) poziomo\n",
    "        canvas = Image.new('RGB', (IMAGE_SIZE*2, IMAGE_SIZE+40), (240,240,240))\n",
    "        canvas.paste(a, (0,0)); canvas.paste(b, (IMAGE_SIZE,0))\n",
    "        d = ImageDraw.Draw(canvas)\n",
    "\n",
    "        # Treść informacyjna\n",
    "        true_label_text = f'PRAWDZIWY: {\"Stego (1)\" if int(lab) == 1 else \"Pseudo (0)\"}'\n",
    "\n",
    "        # Predykcja\n",
    "        predicted_label_int = 1 if score >= chosen_thr else 0\n",
    "        predicted_label_text = f'PRED. {\"Stego (1)\" if predicted_label_int == 1 else \"Pseudo (0)\"}'\n",
    "\n",
    "        # Czy predykcja jest poprawna?\n",
    "        is_correct = (predicted_label_int == int(lab))\n",
    "        text_color = (0, 128, 0) if is_correct else (255, 0, 0)\n",
    "\n",
    "        # Etykiety nad obrazami\n",
    "        d.text((IMAGE_SIZE//2 - 20, 5), 'Obraz A (Cover)', fill=(0,0,0), font=font)\n",
    "        d.text((IMAGE_SIZE + IMAGE_SIZE//2 - 40, 5), 'Obraz B (Stego/Pseudo)', fill=(0,0,0), font=font)\n",
    "\n",
    "\n",
    "        # Tekst pod obrazami (Metryki)\n",
    "        txt = (\n",
    "            f'{true_label_text} | '\n",
    "            f'{predicted_label_text} | '\n",
    "            f'Prawd. Stego: {score:.3f} | '\n",
    "            f'Werdykt: {\"ZGODNY\" if is_correct else \"BŁĘDNY\"}'\n",
    "        )\n",
    "        d.text((10, IMAGE_SIZE+5), txt, fill=text_color, font=font)\n",
    "\n",
    "        # Zapis do pliku\n",
    "        filename_prefix = 'Poprawna' if is_correct else 'Bledna'\n",
    "        canvas.save(os.path.join(epoch_dir, f'przyklad_{filename_prefix}_{k}_lab_{int(lab)}.png'))\n",
    "\n",
    "    print(f'Zapisano {n_save} przykładowych wizualizacji dla Epoki {epoch} do: {epoch_dir}')\n",
    "\n",
    "\n",
    "# ---- TRAIN LOOP ----\n",
    "best_auc = 0.0\n",
    "history = {'train_loss':[], 'val_auc':[]}\n",
    "\n",
    "print('\\n=== Rozpoczęcie Treningu (Stego vs Pseudo) ===')\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    print(f'EP. {epoch}/{NUM_EPOCHS}')\n",
    "    tr_loss = train_epoch(model, train_loader, opt)\n",
    "\n",
    "    # Walidacja\n",
    "    ys_val, preds_val, emb_val, val_paths = eval_model(model, val_loader)\n",
    "\n",
    "    try:\n",
    "        current_auc = roc_auc_score(ys_val, preds_val)\n",
    "    except Exception:\n",
    "        current_auc = 0.0\n",
    "\n",
    "    history['train_loss'].append(tr_loss); history['val_auc'].append(current_auc)\n",
    "    print(f'Train loss: {tr_loss:.4f} | Val AUC: {current_auc:.4f}')\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Zapis najlepszego modelu i artefaktów\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    # Wybór progu dla Macierzy Pomyłek (Youden)\n",
    "    try:\n",
    "        fpr, tpr, thr = roc_curve(ys_val, preds_val)\n",
    "        j = tpr - fpr; j_idx = np.argmax(j); chosen_thr = thr[j_idx]\n",
    "    except Exception:\n",
    "        chosen_thr = 0.5\n",
    "\n",
    "    # Zawsze wylicz bieżące metryki dla CSV (nawet jeśli nie rekord)\n",
    "    try:\n",
    "        # Ponowne obliczenie dla aktualizacji CSV\n",
    "        precision_temp, recall_temp, _ = precision_recall_curve(ys_val, preds_val)\n",
    "        current_pr_auc = auc(recall_temp, precision_temp)\n",
    "        current_acc = accuracy_score(ys_val, (preds_val >= chosen_thr).astype(int))\n",
    "    except Exception:\n",
    "        current_pr_auc = float('nan')\n",
    "        current_acc = float('nan')\n",
    "\n",
    "\n",
    "    if current_auc >= best_auc: # Zmienione na >= dla zapisu pierwszej epoki\n",
    "        if current_auc > best_auc:\n",
    "            best_auc = current_auc\n",
    "            print(f'-> Osiągnięto nowy rekord AUC: {best_auc:.4f}')\n",
    "\n",
    "        # 1. Zapis modelu\n",
    "        torch.save(model.state_dict(), SAVED_MODEL)\n",
    "        print(f'-> Zapisano najlepszy model do: {SAVED_MODEL}')\n",
    "\n",
    "        # 2. Zapis wszystkich metryk i wizualizacji dla tej (nowej rekordowej lub pierwszej) epoki\n",
    "        save_eval_artifacts(model, val_loader, ys_val, preds_val, val_paths, epoch, chosen_thr)\n",
    "        print(f'-> Zapisano pełen zestaw artefaktów ewaluacyjnych dla Epoki {epoch}')\n",
    "\n",
    "    # Aktualizacja CSV z bieżącymi wynikami epoki (niezależnie od tego, czy była rekordowa)\n",
    "    metrics = {\n",
    "        'epoch':[epoch],\n",
    "        'val_auc_roc':[current_auc],\n",
    "        'val_auc_pr':[current_pr_auc],\n",
    "        'threshold':[chosen_thr],\n",
    "        'accuracy':[current_acc]\n",
    "    }\n",
    "    df_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "    # Zapisz do CSV\n",
    "    csv_exists = os.path.exists(METRICS_CSV)\n",
    "    if not csv_exists and epoch == 1:\n",
    "        df_metrics.to_csv(METRICS_CSV, index=False)\n",
    "    elif csv_exists:\n",
    "        df_metrics.to_csv(METRICS_CSV, mode='a', header=False, index=False)\n",
    "\n",
    "print('\\n=== Trening zakończony ===')\n",
    "print(f'Najlepszy wynik AUC ROC: {best_auc:.4f}')\n",
    "print(f'Wyniki metryk i wizualizacje są w katalogu: {RESULTS_DIR}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
